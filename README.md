# CineScribe_VLM
An AI agent that watches movies with you. Uses local VLM to analyze frames, read subtitles, and generate real-time plot summaries and final commentaries. 一个陪你从头看完电影的 AI Agent。利用本地视觉大模型实时分析画面与字幕，自动生成剧情回顾与解说文案。

CineScribe_VLM_v1
此项目是一个基于 Python 和本地多模态大模型（VLM）的智能视频分析工具。它能够像人类观众一样实时“观看”电脑屏幕上的视频窗口，通过视觉识别和字幕读取，实时记录画面内容，并在积累一定信息后自动生成剧情回顾，最终在视频结束时生成完整的影视解说文案。
此工具专为配合 LM Studio 或其他兼容 OpenAI API 格式的本地推理后端（如 Qwen-VL 系列模型）使用而设计。

主要功能
实时屏幕监控与捕获 程序可以锁定并捕获指定的应用程序窗口（如播放器或浏览器），支持自定义边缘裁切，以排除播放器 UI、黑边或无关弹幕，只保留核心画面和字幕区域。
视觉去重与字幕敏感检测 为了节省算力并提高分析效率，程序内置了视觉去重机制。它会计算当前帧与上一帧的视觉差异度（Image Difference）。算法经过特殊优化，重点监测画面下 1/3 区域（通常是字幕出现的位置）。只有当画面发生显著变化或字幕更新时，才会触发 AI 分析；画面静止时自动跳过。
层级化剧情生成 程序采用三层逻辑来处理视频内容： 第一层：单帧分析。识别当前画面的人物、动作、情感以及底部的中文字幕。 第二层：阶段回顾。每分析一定数量的帧（默认为 12 帧），程序会整合最近的记录，生成一段承上启下的剧情小结，并修正单帧分析中可能存在的误判。 第三层：全片解说。当用户停止分析时，程序会将所有阶段回顾串联，生成一篇逻辑连贯、细节丰富的最终解说文案。
自动流控制 在进行耗时较长的“阶段回顾”时，程序可以模拟按下空格键暂停视频播放，确保 AI 不会错过期间的剧情，待分析完成后自动恢复播放。
实时日志记录 提供可视化的 GUI 界面，实时显示 AI 的观察日志、当前的视觉差异数值以及生成的阶段回顾。同时所有记录会自动保存为本地 TXT 文件。

工作原理与循环逻辑
核心循环（Analysis Loop）运行在一个独立的线程中，具体流程如下：
捕获与预处理 在每一轮循环中，程序调用截图工具截取目标窗口，并根据用户设定的裁切值处理图像。
变化检测（视觉门控） 将处理后的图像与上一帧进行对比。 如果差异值低于设定的阈值（默认 2.5），且连续跳过次数未达到上限，程序判断画面为静止，标记为 SKIP 并休眠较短时间（0.5秒），直接进入下一轮循环。 如果差异值高于阈值，或强制分析计时器触发，则进入下一步。
单帧推理 将图像编码为 Base64 格式，连同上下文提示词（包含上一阶段的回顾和最近几帧的记录）发送给本地 VLM API。模型返回对当前画面的描述和字幕内容。
计数与阶段触发 程序记录单帧结果。如果累计分析的帧数达到设定值（SUMMARY_TRIGGER_COUNT），触发阶段回顾流程： 步骤 A：发送空格键暂停视频。 步骤 B：将最近的历史记录发送给 AI，请求生成阶段总结。 步骤 C：记录总结并存入记忆库。 步骤 D：发送空格键恢复视频播放。
循环等待 根据设定的采样间隔（默认 3 秒）扣除本次处理耗时，进行动态休眠，确保持续监控。
最终结算 当用户点击停止按钮，循环结束。程序将内存中所有的“阶段回顾”打包，发送最后一次请求，生成全片总结报告。

简易使用方式
环境准备 确保已安装 Python 环境。 安装必要的依赖库：tkinter（通常内置）, requests, pyautogui, pillow, pygetwindow (可选，用于窗口选取)。 确保本地已启动 LLM 推理服务（如 LM Studio），并加载了支持视觉的模型（推荐 Qwen3-VL）。
配置连接 打开代码文件，找到顶部的 API 设置区域。 修改 LLM_API_URL 为你的本地服务地址（例如 http://localhost:1234/v1/chat/completions）。 修改 MODEL_ID 为你正在运行的模型名称。
启动程序 运行 python 脚本。 界面启动后，首先点击顶部的“选取”按钮，点击你想要监控的视频播放窗口。
调整与运行 观察左侧的预览图。如果画面包含黑边或不需要的 UI，使用“边缘裁切”的数值框进行调整，点击“刷新预览”确认只保留画面和字幕。 点击“开始”按钮。 程序将开始自动截图分析。你可以通过控制台看到的“视觉差异度”数值来判断程序是否在正常捕捉字幕变化。
结束 视频播放完毕后，点击“结束”按钮。程序将自动生成最终解说文案，并显示在日志框和弹窗中。所有的分析记录也会保存在当前目录下的 txt 日志文件中。

CineScribe VLM v1Pro 
这是一个高级的本地化视频理解 Agent。与v1这种单帧分析工具不同，该版本引入了双模型架构、切片拼接技术。它能够自动监控指定屏幕区域，利用小参数模型提取字幕，大参数模型理解剧情，最终生成连贯的影视解说文案。

核心技术区别

双模型分工架构 (Dual-Model Strategy) 不同于使用单一模型处理所有任务，本项目将任务解耦： OCR 专用通道：使用小参数模型（默认配置为 Qwen-VL-4B）专门处理高分辨率的字幕条拼接图。这极大降低了对显存的需求，同时提高了对模糊字幕的识别率。 VLM 剧情通道：使用大参数模型（默认配置为 Qwen-VL-30B）分析经过 2x2 拼接的剧情画面。大模型不再被强制去读微小的字幕，而是专注于理解人物动作、表情和镜头语言。

切片拼接 (Spatio-Temporal Stitching) 为了解决单帧信息量不足和上下文丢失的问题，本程序实现了两种图像预处理算法： 网格拼接 (Grid Stitching)：将连续采集的 4 帧画面按 2x2 方式拼合成一张大图发送给 VLM。这使得 AI 能够一眼看全约 10 秒内的动态变化，而不是静态切片。 这将有助于提高模型对说话人是谁的理解。纵向拼接 (Vertical Stitching)：将 4 帧画面的底部字幕区域垂直堆叠，形成一张长图发送给 OCR 模型。这有助于 AI 根据上下文纠正 OCR 错误，并自动去除重复的字幕行。

异步并行与后台控制 (Async & Background Control) 非阻塞采集：分析过程在独立线程中异步运行，不会阻塞主程序的屏幕截图循环。 后台窗口控制：摒弃了传统的 PyAutoGUI 模拟按键（要求窗口必须在前台），改用 Windows底层 API (ctypes / PostMessageW)。程序直接向目标窗口的句柄发送空格键指令。这意味着你可以在程序挂机看电影时，将播放器最小化或被其他窗口遮挡，不影响暂停和播放控制。

工作原理与循环逻辑

系统运行在一个主循环和多个异步线程中：
采集与视觉门控 主循环每 2.5 秒截取一次用户框选的区域。 计算当前帧与上一帧的视觉差异度 (Diff)。 同时将全图存入“帧缓冲区”，将底部字幕区域存入“字幕缓冲区”。
批处理触发 (Batch Processing) 当缓冲区积累满 4 帧（约 10 秒）时，主循环立即清空缓冲区并启动一个异步线程。 在异步线程中：
调用 OCR 接口处理字幕拼接图，获取去重后的台词。
调用 VLM 接口处理 2x2 剧情拼接图，结合 OCR 结果生成剧情片段记录。
结果写入共享内存列表，并实时显示在 GUI 上。
阶段回顾与流控制 每当处理完 6 个批次（约 60 秒）后，程序会触发“阶段回顾”。 此时，程序通过 Windows API 向播放器发送暂停指令。 将最近 1 分钟的碎片化记录发送给 VLM 进行逻辑梳理，生成阶段性总结。 总结完成后，再次发送指令恢复播放。
最终结算 用户停止程序后，系统将所有“阶段总结”串联，生成一篇完整的影视解说文案。

使用方式

环境配置 确保安装 Python 以及必要的库：tkinter, requests, pillow, pyautogui, numpy (可选)。 必须在 Windows 操作系统下运行（因依赖 win32 API 进行后台控制）。
模型服务准备 你需要启动两个兼容 OpenAI 格式的本地推理服务（例如使用 LM Studio 启动两个不同端口的 Server，或者同一 Server 支持多模型）：
端口 1234 (OCR)：加载 Qwen-VL-4B 或类似的小型视觉模型。
端口 1234 (VLM，建议区分端口或模型ID)：加载 Qwen-VL-30B 或类似的大型视觉模型。 打开代码文件，在顶部的“配置区域”修改 OCR_API_URL 和 VLM_API_URL 以匹配你的本地地址。
运行步骤 步骤一：运行脚本启动 GUI。 步骤二：点击“框选屏幕区域”，鼠标左键拖拽框选视频播放器的核心画面（尽量避开无关的播放器 UI，但必须包含字幕区域）。 步骤三：点击“启动分析”。 步骤四：程序开始自动截图。你可以看到进度条在跳动。 步骤五：影片结束时，点击“停止并生成报告”，等待数秒后，最终文案将弹出并保存在本地 txt 文件中。

注意事项 后台控制功能依赖于能够接收键盘消息的标准 Windows 窗口。某些自绘 UI 的播放器（如部分网页全屏模式）可能无法响应 PostMessage，此时需保持窗口激活。 请根据显卡显存大小适当调整 BATCH_SIZE (默认 4) 和 VLM_MAX_DIMENSION (默认 1560)。使用默认配置的两个模型，4bit量化，将会需要高达35g以上的显存。你也可以只使用Qwen3-VL-30b一个模型完成OCR后进行视频理解，但这可能会导致较大的延迟。
